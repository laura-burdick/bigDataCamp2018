{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries that we're going to use\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk import casual_tokenize, word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lowercase all text\n",
    "#\n",
    "# @param strings\n",
    "#     An array of sentences (not word tokenized)\n",
    "# @returns an array of lowercased sentences\n",
    "\n",
    "def lowercase(strings):\n",
    "    return [i.lower() for i in strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize into words (using Tweet tokenizer, probably not suitable for non-Tweet text)\n",
    "#\n",
    "# @param strings\n",
    "#     An array of sentences\n",
    "# @returns an array of tokenized sentences (each tokenized sentence is an array, so this returns an array of arrays)\n",
    "\n",
    "def tokenize_tweets(strings):\n",
    "    return [casual_tokenize(i) for i in strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize into words (regular text, not Tweet text)\n",
    "#\n",
    "# @param strings\n",
    "#     An array of sentences\n",
    "# @returns an array of tokenized sentences (each tokenized sentence is an array, so this returns an array of arrays)\n",
    "\n",
    "def tokenize_regular(strings):\n",
    "    return [word_tokenize(i) for i in strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize into sentences (regular text, not Tweet text)\n",
    "#\n",
    "# @param text\n",
    "#     A string of text (not broken into sentences)\n",
    "# @returns an array of sentences\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all punctuation from sentences\n",
    "#\n",
    "# @param strings\n",
    "#     An array of sentences\n",
    "# @returns an array of sentences with no punctuation\n",
    "\n",
    "def remove_punctuation(strings):\n",
    "    stripped = [''.join([i for i in sentence if i not in string.punctuation]) for sentence in strings]\n",
    "    return [i for i in stripped if len(i.strip()) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either remove all digits from sentences or replace them with pound sign\n",
    "#\n",
    "# @param tokenized\n",
    "#     An array of tokenized sentences\n",
    "# @ param replace\n",
    "#     Whether to replace the digits with # or not (default = True)\n",
    "# @returns an array of sentences with digits removed or replaced\n",
    "\n",
    "def remove_digits(tokenized,replace=True):\n",
    "    if replace:\n",
    "        stripped = [[re.sub('[0123456789]','#',word) for word in sentence] for sentence in tokenized]\n",
    "    else:\n",
    "        stripped = [[re.sub('[0123456789]','',word) for word in sentence] for sentence in tokenized]\n",
    "        stripped = [[word for word in sentence if len(word) > 0 ] for sentence in stripped]\n",
    "    return [i for i in stripped if len(i) > 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stem all words\n",
    "#\n",
    "# @param tokenized\n",
    "#     An array of tokenized sentences\n",
    "# @returns an array of tokenized sentences with all of the words stemmed\n",
    "\n",
    "def stem_words(tokenized):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [[stemmer.stem(word) for word in sentence] for sentence in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove words that occur less than a certain number of times\n",
    "#\n",
    "# @param tokenized\n",
    "#     An array of tokenized sentences\n",
    "# @param threshold\n",
    "#     The minimum number of times a word has to occur before it is removed (default = 5)\n",
    "# @returns an array of tokenized sentences with rare words replaced with 'UNK'\n",
    "\n",
    "def remove_rare_words(tokenized,threshold=5):\n",
    "    #count the number of times each word appears in all the sentences\n",
    "    counter = Counter([word for sentence in tokenized for word in sentence])\n",
    "    \n",
    "    #remove words that appear less than the threshold number of times (replace with 'UNK')\n",
    "    return [[word if counter[word] >= threshold else 'UNK' for word in sentence] for sentence in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove links (text that begins with 'http://' or 'https://')\n",
    "#\n",
    "# @param tokenized\n",
    "#     An array of tokenized sentences\n",
    "# @returns an array of tokenized sentences with links replaced with 'LINK'\n",
    "\n",
    "def remove_links(tokenized):\n",
    "    return [[word if word[:7] != 'http://' and word[:8] != 'https://' else 'LINK' for word in sentence] \\\n",
    "            for sentence in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract NER relationships from text\n",
    "# Make sure that you've downloaded the Stanford NER files\n",
    "# This only extracts persons, organizations, and locations\n",
    "#\n",
    "# @param tokenized\n",
    "#     An array of tokenized sentences\n",
    "# @returns an array of sentences where each sentence is a list of tuples of (word, entity label)\n",
    "\n",
    "def extract_ner(tokenized):\n",
    "    # Download these files from https://nlp.stanford.edu/software/\n",
    "    # Make sure the paths are set correctly\n",
    "    st = StanfordNERTagger('/Users/laura/software/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\\\n",
    "                      '/Users/laura/software/stanford-ner/stanford-ner.jar') \n",
    "    return st.tag_sents(tokenized) #Batch processing is important - speeds it up tremendously!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate topics from text using LDA\n",
    "#\n",
    "# @param tokenized\n",
    "#     An array of tokenized sentences\n",
    "# @param num_topics\n",
    "#     The number of topics to calculate\n",
    "# @returns a list of topics and their weighted words\n",
    "\n",
    "def topic_modeling(tokenized,num_topics = 10):\n",
    "    dictionary = corpora.Dictionary(tokenized)\n",
    "    corpus = [dictionary.doc2bow(sentence) for sentence in tokenized]\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "    return lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, let's get some sample tweets! (you can replace this with your own data)\n",
    "strings = twitter_samples.strings('tweets.20150430-223406.json') + \\\n",
    "    twitter_samples.strings('positive_tweets.json') + \\\n",
    "    twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "lowercased = lowercase(strings)\n",
    "tokenized = tokenize_tweets(lowercased)\n",
    "tokenized = remove_links(tokenized)\n",
    "tokenized = remove_rare_words(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Named Entity Recognition\n",
    "ner = extract_ner(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = topic_modeling(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'This is a 1234 bunch of sentences. !!! ??? alks;df Hello there233, world!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = tokenize_sentences(text)\n",
    "strings = remove_punctuation(strings)\n",
    "#strings = remove_digits(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = tokenize_regular(strings)\n",
    "strings = remove_digits(strings,replace=False)\n",
    "strings = stem_words(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at some entities\n",
    "entity = 'PLACE'\n",
    "entities = set()\n",
    "counter = Counter()\n",
    "for sentence in ner:\n",
    "    entityStarted = False\n",
    "    fullEntity = ''\n",
    "    for (word,entityLabel) in sentence:\n",
    "        if entityLabel == entity:\n",
    "            print(entityLabel)\n",
    "        if entityLabel == entity:\n",
    "            if entityStarted:\n",
    "                fullEntity += ' ' + word\n",
    "            else:\n",
    "                fullEntity = word\n",
    "                entityStarted = True\n",
    "        elif entityStarted:\n",
    "            entities.add(fullEntity)\n",
    "            counter[fullEntity] += 1\n",
    "            fullEntity = ''\n",
    "            entityStarted = False\n",
    "if fullEntity != '':\n",
    "    entities.add(fullEntity)\n",
    "    counter[fullEntity] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entities:\n",
    "    print(entity,counter[entity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = topic_modeling(tokenized,num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x121a1ce10>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
